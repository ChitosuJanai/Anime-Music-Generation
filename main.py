import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from music21 import *
from collections import Counter
import io
import pretty_midi
from scipy.io import wavfile
import os


@st.cache_resource
def load_model(model_file):
    model = tf.keras.models.load_model(model_file, compile=False)
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")
    return model


def predict(model, input):
    prediction = model.predict(input)
    return prediction


def read_midi(file):

    print("Loading Music File:", file)

    notes = []
    notes_to_parse = None

    # parsing a midi file
    midi = converter.parse(file)

    # grouping based on different instruments
    s2 = instrument.partitionByInstrument(midi)

    # Looping over all the instruments
    for part in s2.parts:

        # select elements of only piano
        if 'Piano' in str(part):

            notes_to_parse = part.recurse()

            # finding whether a particular element is note or a chord
            for element in notes_to_parse:

                # note
                if isinstance(element, note.Note):
                    notes.append(str(element.pitch))

                # chord
                elif isinstance(element, chord.Chord):
                    notes.append('.'.join(str(n) for n in element.normalOrder))

    return np.array(notes)


def convert_to_midi(prediction_output, file_path):

    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:

        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:

                cn = int(current_note)
                new_note = note.Note(cn)
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)

            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)

        # pattern is a note
        else:

            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=file_path)


def convertToPrediction(file):
    with st.spinner(f"Transcribing to FluidSynth"):
        notes_array = np.array([read_midi(file)])

        # Assuming notes_array is your new music dataset
        notes_ = [element for note_ in notes_array for element in note_]

        # Computing the frequency of each note
        freq = dict(Counter(notes_))

        # Considering only the frequencies
        no = [count for _, count in freq.items()]

        frequent_notes = [note_ for note_,
                          count in freq.items() if count >= 10]

        new_music = []

        for notes in notes_array:
            temp = []
            for note_ in notes:
                if note_ in frequent_notes:
                    temp.append(note_)
            new_music.append(temp)

        new_music = np.array(new_music)

        no_of_timesteps = 32
        x = []
        y = []

        for note_ in new_music:
            for i in range(0, len(note_) - no_of_timesteps, 1):
                # Preparing input and output sequences
                input_ = note_[i:i + no_of_timesteps]
                output = note_[i + no_of_timesteps]

                x.append(input_)
                y.append(output)

        x = np.array(x)
        y = np.array(y)

        unique_x = list(set(x.ravel()))
        x_note_to_int = dict((note_, number)
                             for number, note_ in enumerate(unique_x))

        # Preparing input sequences
        x_seq = []
        for i in x:
            temp = []
            for j in i:
                # Assigning unique integer to every note
                temp.append(x_note_to_int[j])
            x_seq.append(temp)

        x_seq = np.array(x_seq)

        # You can now use the 'no' list for your selected music
        notes_count = 0
        if len(x_seq) == 1:
            st.divider()
            notes_count = st.slider(
                'Select how many notes that you want to generate', 30, 360, 60, step=30)
            st.divider()
            st.write("Generating music...")
            selected_music_index = 0
        elif len(x_seq) > 1:
            st.divider()
            notes_count = st.slider(
                'Select how many notes that you want to generate', 30, 360, 60, step=30)
            st.divider()
            st.write("Generating music...")
            selected_music_index = np.random.randint(0, len(x_seq) - 1)
        else:
            print('No music found')
            st.divider()
            st.write('Piano notes not found or corpus error.')
            return

        selected_music = x_seq[selected_music_index]

        # Generate variations or enhance the selected music
        variations = []
        for i in range(notes_count):
            selected_music_reshaped = selected_music.reshape(
                1, no_of_timesteps)

            # Use the model to generate predictions or variations with temperature
            temperature = 0.5  # Adjust the temperature as needed
            prob = model.predict(selected_music_reshaped)[0]
            y_pred = np.random.choice(len(prob), p=prob)

            variations.append(y_pred)

            # Apply variations to the selected music
            selected_music = np.insert(
                selected_music, len(selected_music), y_pred)
            selected_music = selected_music[1:]

        x_int_to_note = {number: note_ for number, note_ in enumerate(notes_)}
        variations_notes = [x_int_to_note[i] for i in variations]

        convert_to_midi(variations_notes, 'test_output.mid')
        midi_data = pretty_midi.PrettyMIDI('test_output.mid')
        os.unlink('test_output.mid')
        audio_data = midi_data.fluidsynth()
        audio_data = np.int16(
            audio_data / np.max(np.abs(audio_data)) * 32767 * 0.9
        )  # -- Normalize for 16 bit audio https://github.com/jkanner/streamlit-audio/blob/main/helper.py

        virtualfile = io.BytesIO()
        wavfile.write(virtualfile, 44100, audio_data)

    st.audio(virtualfile)
    st.markdown("Download the audio by right-clicking on the media player")


notes_ = []
x_int_to_note = {number: note_ for number, note_ in enumerate(notes_)}
st.title("Mushit Generator")
model = load_model('model.h5')
file = st.file_uploader("Upload a midi", type="mid",
                        accept_multiple_files=False)
if file is not None:
    st.write("File uploaded")
    bytes_data = file.getvalue()

    convertToPrediction(bytes_data)
